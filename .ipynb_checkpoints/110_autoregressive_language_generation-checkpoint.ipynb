{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae55c84f",
   "metadata": {},
   "source": [
    "# Autoregressive (자동회귀) 문장 생성\n",
    "\n",
    "간단한 자동 회귀적인 텍스트 생성을 위한 코드 예제입니다. 여기서는 OpenAI의 GPT-2 모델을 사용하는데, GPT-3은 API를 통해서만 직접 사용할 수 있기 때문입니다. 그럼에도 GPT-2는 GPT-3와 매우 유사한 구조를 가지고 있으므로, 동일한 접근 방식을 사용합니다.\n",
    "\n",
    "### Colab에서 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f48a4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\trimu\\anaconda3\\lib\\site-packages (4.33.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\trimu\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\trimu\\anaconda3\\lib\\site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\trimu\\anaconda3\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\trimu\\anaconda3\\lib\\site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\trimu\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\trimu\\anaconda3\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: requests in c:\\users\\trimu\\anaconda3\\lib\\site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\trimu\\anaconda3\\lib\\site-packages (from transformers) (0.3.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\trimu\\anaconda3\\lib\\site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in c:\\users\\trimu\\anaconda3\\lib\\site-packages (from transformers) (0.17.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\trimu\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.7.1)\n",
      "Requirement already satisfied: fsspec in c:\\users\\trimu\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2022.7.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\trimu\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\trimu\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.5)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\trimu\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\trimu\\anaconda3\\lib\\site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\trimu\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\trimu\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\trimu\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\trimu\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\trimu\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\trimu\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\trimu\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\trimu\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00b75689",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[7454, 2402,  257,  640]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# 모델 및 토크나이저 초기화\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2-medium\")  \n",
    "\n",
    "# 문장 시작 부분\n",
    "input_text = \"Once upon a time\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92f576b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7454,  2402,   257,   640,    11,   612,   373,   257,   582,   508,\n",
       "          5615,   287,   257,  7404,  1444,   509, 17716,   322,    13,   679,\n",
       "           373,   257,   845,   922,   582,    11,   290,   339,   373,   845,\n",
       "          1611,   284,   465,  1751,    13,  1881,  1110,    11,   339,   373,\n",
       "          6155,  1863,   262,  2975,    11,   290,   339,  2497,   257,  2415]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문장 생성\n",
    "output = model.generate(input_ids, max_length=50, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "967494d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text 1: Once upon a time, there was a man who lived in a village called Krakow. He was a very good man, and he was very kind to his children. One day, he was walking along the road, and he saw a woman\n"
     ]
    }
   ],
   "source": [
    "# 생성된 문장 출력\n",
    "for i, generated_text in enumerate(output):\n",
    "    decoded_text = tokenizer.decode(generated_text, skip_special_tokens=True)\n",
    "    print(f\"Generated text {i + 1}: {decoded_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10e2ff5",
   "metadata": {},
   "source": [
    "GPT-2는 자체적으로 autoregressive 모델입니다. \"Autoregressive\"란, 이전에 생성된 토큰들을 기반으로 다음 토큰을 생성하는 모델을 의미합니다.\n",
    "\n",
    "위의 코드에서 `model.generate` 메서드는 이미 autoregressive한 방식으로 문장을 생성합니다. 그러나 이를 명시적으로 보여주기 위해 각 단계에서 토큰을 하나씩 생성하는 autoregressive한 코드를 아래에 작성하겠습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c8ab4dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[7454, 2402,  257,  640]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문장 시작 부분\n",
    "input_text = \"Once upon a time\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5ba82e",
   "metadata": {},
   "source": [
    "input_ids를 입력으로 받아 next token의 확률 분포를 예측값으로 반환 받습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84a6c12f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ -74.6887,  -74.2220,  -80.2354,  ...,  -83.7431,  -80.4567,\n",
       "           -76.2926],\n",
       "         [-225.0849, -225.3535, -229.2500,  ..., -236.9878, -237.3179,\n",
       "          -224.3200],\n",
       "         [  -6.1224,   -4.9656,  -10.4818,  ...,  -13.5871,  -12.2557,\n",
       "            -5.6445],\n",
       "         [  -9.2671,   -7.4639,  -14.5420,  ...,  -16.8138,  -16.4736,\n",
       "            -9.4272]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model(input_ids)\n",
    "logits = predictions.logits\n",
    "logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d59498",
   "metadata": {},
   "source": [
    "예측값 중 가장 큰 값의 token을 선택합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83bc0992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측한 next token : 11\n",
      "decoding한 next token : ,\n"
     ]
    }
   ],
   "source": [
    "predicted_token = torch.argmax(logits[0, -1]).item()\n",
    "\n",
    "print(f\"예측한 next token : {predicted_token}\")\n",
    "token = tokenizer.decode(predicted_token, skip_special_tokens=True)\n",
    "print(f\"decoding한 next token : {token}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50ff737",
   "metadata": {},
   "source": [
    "위와 같은 방식으로 마지막 단어 이어 붙이기 형식의 문장 생성을 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74339413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[7454, 2402,  257,  640,   11]])\n",
      "tensor([[7454, 2402,  257,  640,   11,  612]])\n",
      "tensor([[7454, 2402,  257,  640,   11,  612,  373]])\n",
      "tensor([[7454, 2402,  257,  640,   11,  612,  373,  257]])\n",
      "tensor([[7454, 2402,  257,  640,   11,  612,  373,  257,  582]])\n",
      "tensor([[7454, 2402,  257,  640,   11,  612,  373,  257,  582,  508]])\n",
      "tensor([[7454, 2402,  257,  640,   11,  612,  373,  257,  582,  508, 5615]])\n",
      "tensor([[7454, 2402,  257,  640,   11,  612,  373,  257,  582,  508, 5615,  287]])\n",
      "tensor([[7454, 2402,  257,  640,   11,  612,  373,  257,  582,  508, 5615,  287,\n",
      "          257]])\n",
      "tensor([[7454, 2402,  257,  640,   11,  612,  373,  257,  582,  508, 5615,  287,\n",
      "          257, 7404]])\n",
      "tensor([[7454, 2402,  257,  640,   11,  612,  373,  257,  582,  508, 5615,  287,\n",
      "          257, 7404, 1444]])\n",
      "tensor([[7454, 2402,  257,  640,   11,  612,  373,  257,  582,  508, 5615,  287,\n",
      "          257, 7404, 1444,  509]])\n",
      "tensor([[ 7454,  2402,   257,   640,    11,   612,   373,   257,   582,   508,\n",
      "          5615,   287,   257,  7404,  1444,   509, 17716]])\n",
      "tensor([[ 7454,  2402,   257,   640,    11,   612,   373,   257,   582,   508,\n",
      "          5615,   287,   257,  7404,  1444,   509, 17716,   322]])\n",
      "tensor([[ 7454,  2402,   257,   640,    11,   612,   373,   257,   582,   508,\n",
      "          5615,   287,   257,  7404,  1444,   509, 17716,   322,    13]])\n",
      "tensor([[ 7454,  2402,   257,   640,    11,   612,   373,   257,   582,   508,\n",
      "          5615,   287,   257,  7404,  1444,   509, 17716,   322,    13,   679]])\n",
      "Once upon a time, there was a man who lived in a village called Krakow. He\n"
     ]
    }
   ],
   "source": [
    "# Autoregressive한 방식으로 문장 생성\n",
    "max_length = 20                     # 생성할 문장의 최대 길이 설정\n",
    "input_ids_concat = input_ids  # 초기 입력 토큰(예: 문장의 시작 부분)\n",
    "\n",
    "# 생성할 문장의 길이가 max_length보다 작을 동안 반복\n",
    "while input_ids_concat.shape[1] < max_length:\n",
    "    # 모델을 사용하여 다음 토큰 예측\n",
    "    predictions = model(input_ids_concat)  \n",
    "    logits = predictions.logits                  # 예측된 로짓을 가져옴\n",
    "    predicted_token = torch.argmax(logits[0, -1]).item()  # 로짓에서 가장 확률이 높은 토큰을 선택\n",
    "    \n",
    "    # 예측된 토큰을 기존의 입력 토큰 뒤에 추가\n",
    "    input_ids_concat = torch.cat([input_ids_concat, torch.tensor([[predicted_token]])], dim=1)  \n",
    "    print(input_ids_concat)   # 현재까지 예측된 토큰들을 출력\n",
    "\n",
    "# 최종적으로 생성된 문장 출력\n",
    "decoded_text = tokenizer.decode(input_ids_concat[0], skip_special_tokens=True)  \n",
    "print(decoded_text)  # 디코딩하여 텍스트 형태로 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573d7029",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
