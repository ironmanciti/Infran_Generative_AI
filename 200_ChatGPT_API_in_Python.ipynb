{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2e08433",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e2e08433",
    "outputId": "fce3a2bb-9f21-4e6d-bc8f-cad066ece3bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Downloading openai-1.7.1-py3-none-any.whl (224 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.9/224.9 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
      "Collecting httpx<1,>=0.23.0 (from openai)\n",
      "  Downloading httpx-0.26.0-py3-none-any.whl (75 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.13)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
      "Collecting typing-extensions<5,>=4.7 (from openai)\n",
      "  Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.11.17)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
      "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: typing-extensions, h11, httpcore, httpx, openai\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.5.0\n",
      "    Uninstalling typing_extensions-4.5.0:\n",
      "      Successfully uninstalled typing_extensions-4.5.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "llmx 0.0.15a0 requires cohere, which is not installed.\n",
      "llmx 0.0.15a0 requires tiktoken, which is not installed.\n",
      "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed h11-0.14.0 httpcore-1.0.2 httpx-0.26.0 openai-1.7.1 typing-extensions-4.9.0\n",
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -q openai\n",
    "!pip install -q python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42ca44b1",
   "metadata": {
    "id": "42ca44b1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import sys\n",
    "sys.path.append('./')\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "openai.api_key  = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a511e7",
   "metadata": {
    "id": "22a511e7"
   },
   "source": [
    "# Method 1 - openai library 이용\n",
    "## role of the messages author\n",
    "\n",
    "OpenAI API에서 각 메시지의 저자(`author`) 역할은 다음과 같은 의미를 가집니다:\n",
    "\n",
    "1. **system**: 시스템 관련 메시지나 지침. 예를 들면, 어시스턴트에게 특정 작업을 수행하는 방법에 대한 지침을 제공하는 경우에 사용됩니다. 이러한 지침은 어시스턴트의 응답을 안내하거나 제한할 수 있습니다.\n",
    "\n",
    "2. **user**: 사용자가 입력한 메시지. 대체로 사용자의 질문, 요청, 지침 등을 나타냅니다.\n",
    "\n",
    "3. **assistant**: 어시스턴트(즉, 모델)가 반환하는 응답 또는 메시지. 사용자의 질문에 대한 답변, 제안, 설명 등을 포함할 수 있습니다.\n",
    "\n",
    "4. **function**: API의 이전 버전에서 사용됐던 것으로, 특정 기능이나 동작을 어시스턴트에게 요청할 때 사용되었습니다. 최신 버전에서는 일반적으로 사용되지 않습니다.\n",
    "\n",
    "이러한 역할은 대화형 세션에서 메시지의 발신자와 의도를 구별하는 데 도움을 줍니다. API는 이 정보를 사용하여 적절한 방식으로 반응하거나 응답합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0eecb89-f9f4-4ed5-a364-1a92f4df23c1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a0eecb89-f9f4-4ed5-a364-1a92f4df23c1",
    "outputId": "af60928a-a0ed-48b3-b17c-a832950d6868"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a test."
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "stream = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}],\n",
    "    stream=True,\n",
    ")\n",
    "for chunk in stream:\n",
    "    if chunk.choices[0].delta.content is not None:\n",
    "        print(chunk.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1fefb54",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a1fefb54",
    "outputId": "ea421623-ba84-4582-9c78-9b811c984dd3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User : 너는 누구야\n",
      "ChatGPT: 저는 인공지능 기반의 도움을 주는 개인 비서입니다. 어떤 도움이 필요하신가요?\n",
      "User : 강남에서 남산에 가는 길을 알려줘\n",
      "ChatGPT: 강남에서 남산으로 가는 길은 여러 가지 방법이 있습니다. 가장 편리한 방법은 지하철을 이용하는 것인데요, 다음은 그 방법에 대한 안내입니다.\n",
      "\n",
      "1. 강남역 (2호선)에서 노량진 방향으로 지하철을 탑니다.\n",
      "2. 3개 역을 지나 압구정역에 도착한 뒤, 6번 출구로 나옵니다.\n",
      "3. 압구정역에서 남산 방향으로 택시를 타거나 도보로 이동합니다. 걸어서는 약 30분 정도 소요됩니다.\n",
      "\n",
      "다른 대안으로는 버스를 이용하는 것이 있습니다. 강남역 근처 버스 정류장에서 버스를 타고 남산으로 이동할 수 있습니다. 정확한 버스 번호와 정류장을 알려드리려면 출발 지점과 도착 지점의 구체적인 위치를 알려주세요.\n",
      "\n",
      "또한, 남산에는 주차 공간이 제한되어 있으니 차량을 이용하실 경우 사전에 주변 주차장 예약과 관련된 사항도 고려해야 합니다.\n",
      "\n",
      "도움이 되셨나요? 추가적인 도움이 필요하시면 언제든지 말씀해주세요.\n",
      "User : quit\n",
      "ChatGPT: 알겠습니다. 궁금한 점이 있으면 언제든지 도움을 요청해주세요. 안녕히 가세요!\n",
      "User : stop\n"
     ]
    }
   ],
   "source": [
    "# assistant 에 대한 지침이 되는 context 정보를 제공\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "]\n",
    "\n",
    "while True:\n",
    "    # 사용자로부터 메시지 입력 받기\n",
    "    message = input(\"User : \")\n",
    "\n",
    "    # 사용자가 'stop'이라고 입력하면 대화 종료\n",
    "    if message.lower() == \"stop\":\n",
    "        break\n",
    "    else:\n",
    "        # 사용자의 메시지를 메시지 목록에 추가\n",
    "        messages.append(\n",
    "             {\"role\": \"user\", \"content\": message}\n",
    "        )\n",
    "\n",
    "        # OpenAI API를 사용하여 대화형 완성 진행\n",
    "        completion = openai.chat.completions.create(\n",
    "          model=\"gpt-3.5-turbo\",\n",
    "          messages=messages\n",
    "        )\n",
    "\n",
    "    # 모델의 답변 추출\n",
    "    reply = completion.choices[0].message.content\n",
    "\n",
    "    # 모델의 답변 출력\n",
    "    print(f\"ChatGPT: {reply}\")\n",
    "\n",
    "    # 모델의 답변을 메시지 목록에 추가\n",
    "    messages.append({\"role\": \"assistant\", \"content\": reply})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d9105d",
   "metadata": {
    "id": "e9d9105d"
   },
   "outputs": [],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a589797d",
   "metadata": {
    "id": "a589797d"
   },
   "source": [
    "# Method 2 - API Endpoint 이용\n",
    "\n",
    "payload\"는 주로 HTTP 요청을 보낼 때 전달되는 데이터를 의미합니다. API 호출에서 \"payload\"는 주로 POST 요청의 본문에 포함된 데이터를 나타냅니다.  \n",
    "\n",
    "- temperature : 사용할 샘플링 온도는 0에서 2 사이입니다. 0.8과 같이 값이 높을수록 출력이 더 무작위로 생성(Hallucination 발생 가능)되고, 0.2와 같이 값이 낮을수록 더 집중적이고 결정적이게 됩니다.\n",
    "\n",
    "- top_p : temperature의 대안으로, 확률 top_p인 토큰의 결과를 고려합니다. 따라서 0.1은 상위 10% 확률을 구성하는 토큰만 고려된다는 의미입니다.\n",
    "\n",
    "**temperature 와 top_p 중 하나만 변경하고, 둘 다 변경하지는 않는 것이 좋습니다.**  \n",
    "\n",
    "- n:  각 입력 메시지에 대해 생성할 chat completion 선택 수입니다.  \n",
    "\n",
    "-  presence_penalty : -2.0과 2.0 사이의 숫자입니다. 양수 값은 지금까지 텍스트에 나타나는지 여부에 따라 새 토큰에 불이익을 주어 모델이 새로운 주제에 관해 이야기할 가능성을 높입니다.\n",
    "\n",
    "- frequency_penalty : -2.0과 2.0 사이의 숫자입니다. 양수 값은 지금까지 텍스트의 기존 빈도를 기반으로 새 토큰에 불이익을 주어 모델이 동일한 줄을 그대로 반복할 가능성을 줄입니다.  \n",
    "\n",
    "Bearer는 HTTP 인증 스키마 중 하나입니다. 웹 서비스에서 사용하는 토큰 기반의 인증 방식 중 가장 일반적인 방식입니다. Bearer를 사용하면, 클라이언트는 해당 토큰을 포함하여 서버에 요청을 보낼 수 있으며, 서버는 이 토큰을 검증하여 해당 클라이언트의 요청을 인증합니다. Bearer {토큰} 형식에서 {토큰} 부분은 실제 API를 사용하기 위한 인증 토큰을 나타냅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2cb4c5b8",
   "metadata": {
    "id": "2cb4c5b8"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# OpenAI API의 URL을 정의\n",
    "URL = \"https://api.openai.com/v1/chat/completions\"\n",
    "\n",
    "# 요청에 필요한 데이터를 payload 변수에 저장\n",
    "payload = {\n",
    "    \"model\": \"gpt-3.5-turbo\",\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": f\"세계 바둑 Champion이 누구야 ?\"}],\n",
    "    \"temperature\": 1.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"n\": 1,\n",
    "    \"stream\": False,     # 스트림모드 사용 여부\n",
    "    \"presence_penalty\": 0,  # 결과의 일관성에 영향\n",
    "    \"frequency_penalty\": 0,  # 단어의 사용 빈도에 영향\n",
    "}\n",
    "\n",
    "# 요청 헤더에 내용 유형 및 인증 키 설정\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",  # 요청 본문의 유형을 JSON으로 지정\n",
    "    \"Authorization\": f\"Bearer {openai.api_key}\"  # 인증 키 포함\n",
    "}\n",
    "\n",
    "# requests.post를 사용하여 OpenAI API에 POST 요청을 보냅니다.\n",
    "response = requests.post(URL, headers=headers, json=payload, stream=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfe7af8f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bfe7af8f",
    "outputId": "ef670355-5be9-485c-905b-0aecaa193f5c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa0d95d",
   "metadata": {
    "id": "0aa0d95d"
   },
   "source": [
    "### response 내용\n",
    "1. **choices**: API 응답의 주요 내용이 포함된 배열입니다.\n",
    "   - **finish_reason**: 응답이 완료된 이유입니다. 'stop'은 출력이 종료된 이유가 됩니다.\n",
    "   - **index**: 선택 사항의 인덱스 번호입니다.\n",
    "   - **message**: 사용자 또는 보조프로그램의 메시지 내용입니다.\n",
    "     - **content**: 보조프로그램이 생성한 메시지 내용입니다.\n",
    "     - **role**: 이 메시지를 생성한 주체입니다.  'assistant'는 메시지가 OpenAI의 assistena program으로부터 생성되었다는 것을 의미합니다.\n",
    "\n",
    "2. **created**: 응답이 생성된 시간의 타임스탬프입니다.\n",
    "\n",
    "3. **id**: 응답에 할당된 고유 식별자입니다.\n",
    "\n",
    "4. **model**: 사용된 모델의 이름입니다.\n",
    "\n",
    "5. **object**: 이 객체의 유형을 나타냅니다. 'chat.completion'은 채팅 완료 응답임을 나타냅니다.\n",
    "\n",
    "6. **usage**: 이 요청에서 사용된 토큰의 수를 나타내는 정보입니다.\n",
    "   - **completion_tokens**: 응답 내용에서 사용된 토큰 수입니다.\n",
    "   - **prompt_tokens**: 사용자의 원래 질문에서 사용된 토큰 수입니다.\n",
    "   - **total_tokens**: 전체 토큰 수입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88f59c2f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "88f59c2f",
    "outputId": "fcfe01e2-5612-4b5c-96cd-3952912f4336"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'choices': [{'finish_reason': 'stop',\n",
      "              'index': 0,\n",
      "              'logprobs': None,\n",
      "              'message': {'content': '현재 세계 바둑 챔피언은 알파고(AI 프로그램) 입니다. 그 이전에는 '\n",
      "                                     '이세돌 9단과 이란의 압도적인 강자인 알리 자데 모라디아바디가 세계 바둑 '\n",
      "                                     '챔피언으로 알려져 있습니다.',\n",
      "                          'role': 'assistant'}}],\n",
      " 'created': 1704932718,\n",
      " 'id': 'chatcmpl-8fdAkomElPoJt1rNobGftva6jpL9g',\n",
      " 'model': 'gpt-3.5-turbo-0613',\n",
      " 'object': 'chat.completion',\n",
      " 'system_fingerprint': None,\n",
      " 'usage': {'completion_tokens': 100, 'prompt_tokens': 22, 'total_tokens': 122}}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "import json\n",
    "\n",
    "# 응답 내용을 JSON으로 파싱\n",
    "response_json = response.json()\n",
    "\n",
    "# 파싱된 JSON 내용을 pretty-print로 출력\n",
    "pprint.pprint(response_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "354fba90",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "354fba90",
    "outputId": "878d0307-fa78-4de8-a943-cbac68d1ca29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 세계 바둑 챔피언은 알파고(AI 프로그램) 입니다. 그 이전에는 이세돌 9단과 이란의 압도적인 강자인 알리 자데 모라디아바디가 세계 바둑 챔피언으로 알려져 있습니다.\n"
     ]
    }
   ],
   "source": [
    "print(response_json['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b0b2a4",
   "metadata": {
    "id": "a6b0b2a4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
