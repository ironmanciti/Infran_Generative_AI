{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n2e_xO6rgVc3"
   },
   "source": [
    "# Simplified Vision Transformer\n",
    "\n",
    "ViT를 이용한 CIFAR-100(Canadian Institute for Advanced Research, 총 100개의 클래스 또는 범주) 이미지 분류  \n",
    "\n",
    "### GPU 필요하므로 Colab에서 실행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3brxMeCSgAVm"
   },
   "source": [
    "CIFAR-100 데이터셋은 이미지 크기가 32x32로 상대적으로 작은 이미지로 구성되어 있습니다. 따라서 Vision Transformer 모델을 이 데이터셋에 적용하려면 32x32 이미지를 224x224 크기로 변경하여 패치로 분할하는 것이 일반적입니다. 이렇게 하면 모델이 더 큰 패치에서 특징을 추출하고 이미지 내의 다양한 정보를 고려할 수 있게 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aojI6su5JiK5"
   },
   "source": [
    " `tf.image.extract_patches`는 주어진 이미지에서 작은 패치들을 추출  \n",
    "\n",
    "1. **images**:  `[batch size, height, width, channels]`\n",
    "2. **sizes**: 추출하고자 하는 패치의 크기. `[1, size_rows, size_cols, 1]`의 shape.  \n",
    "    첫번째 1: batch dimension. 이 함수에서는 한 배치 내에서 한 이미지만 처리하기 때문에 1로 설정.    \n",
    "    size_rows: 패치의 행 크기.  \n",
    "    size_cols: 패치의 열 크기.  \n",
    "    마지막 1: 이미지의 채널 수. tf.image.extract_patches 함수에서는 모든 채널에 대한 패치를 동시에 추출하기 때문에 이 값은 항상 1로 설정.\n",
    "\n",
    "3. **strides**: 두 연속적인 패치의 중심 간의 간격을 나타내는 4차원 텐서입니다. `[1, stride_rows, stride_cols, 1]`의 형태를 가지며, 실질적으로 `stride_rows`와 `stride_cols`만 주요하게 사용됩니다.\n",
    "\n",
    "4. **rates**: 패치 추출 시 샘플링 간격을 조절.  `[1, 1, 1, 1]` 값은 일반적인 샘플링을 의미.\n",
    "\n",
    "5. **padding**:  'VALID'는 이미지 내에서 완전히 포함된 패치만 포함\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3CNcGkASYfcm"
   },
   "source": [
    "### 학습 데이터 시각화\n",
    "본래의 32X32 image를 224X224 로 resize 하였으므로 사진이 흐릿하게 보일것임."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IkvAJD1qg7Xg"
   },
   "source": [
    "### PatchEncoder 정의\n",
    "- Vision Transformer (ViT) 모델의 핵심 구성 요소 중 하나인 PatchEncoder 클래스를 정의  \n",
    "- 이 클래스는 패치를 인코딩하고 위치 정보를 임베딩"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AVz4sfNeiWT7"
   },
   "source": [
    "<img src=\"https://viso.ai/wp-content/uploads/2021/09/vision-transformer-vit.png\" width=700 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p15yzD57lRUh"
   },
   "source": [
    "Top-k 정확도는 다중 클래스 분류 문제에서 모델의 성능을 측정하는 지표 중 하나로, 모델이 예측한 클래스 중 상위 k개의 예측 중에 정답 클래스가 포함된 비율.  주로 이미지 분류와 같이 클래스 수가 많고 모델이 상위 몇 개의 가능성 있는 클래스를 예측할 수 있는 경우에 유용.\n",
    "\n",
    "11min 50s 예상"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vRyFVe8qnNcR"
   },
   "source": [
    "### 논문의 benchmark 결과에 의하면 Cifar100 에서 94% 이상의 accuracy 달성"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
